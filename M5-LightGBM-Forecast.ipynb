{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f661713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d28bb74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data files\n",
    "data = pd.read_csv(\"C:/Data/M5store_2.csv\")\n",
    "\n",
    "# Ensure proper formatting\n",
    "data['ds'] = pd.to_datetime(data['d'])\n",
    "data = data.sort_values(by=['store_id', 'ds']).reset_index(drop=True)\n",
    "\n",
    "data.rename(columns={'revenue': 'y'}, inplace=True)\n",
    "\n",
    "data['lagged_1_price'] = data['sell_price'].shift(1)\n",
    "data['lagged_3_price'] = data['sell_price'].shift(3)\n",
    "data['lagged_7_price'] = data['sell_price'].shift(7)\n",
    "\n",
    "#data['avg_price'] = data['y'] / data['sales']\n",
    "#data['lagged_1_avg_price'] = data['avg_price'].shift(1)\n",
    "#data['lagged_3_avg_price'] = data['avg_price'].shift(3)\n",
    "#data['lagged_7_avg_price'] = data['avg_price'].shift(7)\n",
    "#data.drop(columns=['avg_price'], inplace=True)\n",
    "#data.drop(columns=['sales'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0857b422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate lag and rolling features\n",
    "def create_features(df, lags, rolling_windows):\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df.groupby('store_id')['y'].shift(lag)\n",
    "    for window in rolling_windows:\n",
    "        df[f'roll_mean_{window}'] = df.groupby('store_id')['y'].shift(1).rolling(window=window).mean()\n",
    "        df[f'roll_std_{window}'] = df.groupby('store_id')['y'].shift(1).rolling(window=window).std()\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "lags = [7, 14, 21, 28, 364]\n",
    "rolling_windows = [7, 14, 28]\n",
    "data = create_features(data, lags, rolling_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "81bff39d-8fe7-4b21-87ab-4008b46a4a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate rolling slopes\n",
    "def calculate_rolling_slope(series, window):\n",
    "    slopes = [np.nan] * (window - 1)  # Fill with NaN for the first (window - 1) rows\n",
    "    regressor = LinearRegression()\n",
    "\n",
    "    for i in range(len(series) - window + 1):\n",
    "        y = series[i:i + window].values  # Sales in the rolling window\n",
    "        x = np.arange(window).reshape(-1, 1)  # Time indices for the window\n",
    "        regressor.fit(x, y)\n",
    "        slopes.append(regressor.coef_[0])  # Extract the slope\n",
    "\n",
    "    return slopes\n",
    "\n",
    "# Group by store_id and calculate rolling slopes\n",
    "window_size = 3  # Specify the rolling window size\n",
    "data['rolling_sales_slope'] = data.groupby('store_id')['y'].transform(\n",
    "    lambda x: calculate_rolling_slope(x, window_size)\n",
    ")\n",
    "\n",
    "data['lagged_1_rolling_sales_slope'] = data['rolling_sales_slope'].shift(1)\n",
    "data.drop(columns=['rolling_sales_slope'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1808f5d2-5f8b-499b-81d9-e3061b2d7a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract temporal features\n",
    "data['month'] = data['ds'].dt.month  # Month of the year (1-12)\n",
    "data['day_of_month'] = data['ds'].dt.day  # Day of the month (1-31)\n",
    "data['day_of_week'] = data['ds'].dt.weekday  # Day of the week (0=Monday, 6=Sunday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "397c1d3e-b104-4179-a2b7-795b921f9540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Thanksgiving\n",
    "# Thanksgiving is the 4th Thursday in November in the US\n",
    "data['is_thanksgiving'] = data['ds'].apply(lambda x: (x.month == 11) and (x.weekday() == 3) and (15 < x.day <= 28))\n",
    "\n",
    "# Encode Christmas\n",
    "# Christmas is always on December 25\n",
    "data['is_christmas'] = data['ds'].apply(lambda x: (x.month == 12) and (x.day == 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "66720758-93b3-48a4-8f57-697ece2f930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pivot table\n",
    "pivot_data = data.pivot(index='ds', columns='store_id', values='y')\n",
    "pivot_data.columns = [f'sales_store_{store}' for store in pivot_data.columns]\n",
    "\n",
    "# Merge the pivoted data with the original dataset\n",
    "data = pd.merge(\n",
    "    data, \n",
    "    pivot_data.reset_index(), \n",
    "    on=['ds'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop self-sales for each store\n",
    "for store in data['store_id'].unique():\n",
    "    feature_to_drop = f'sales_store_{store}'\n",
    "    data.loc[data['store_id'] == store, feature_to_drop] = None\n",
    "\n",
    "# Create lagged values for all columns starting with 'sales_store_'\n",
    "for col in [col for col in data.columns if col.startswith('sales_store_')]:\n",
    "    # Replace the column with its lagged version\n",
    "    data[col] = data.groupby('store_id')[col].shift(1)  # Replace '1' with desired lag period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2aa97758-5edf-4382-a15e-f7b0a2977c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total demand across all stores for each date\n",
    "total_demand = data.groupby('ds')['y'].sum().reset_index()\n",
    "total_demand.rename(columns={'y': 'total_sales_all_stores'}, inplace=True)\n",
    "\n",
    "# Merge total demand back into the original data\n",
    "data = pd.merge(data, total_demand, on='ds', how='left')\n",
    "\n",
    "# Subtract the current store's sales to get demand from other stores\n",
    "data['demand_other_stores'] = data['total_sales_all_stores'] - data['y']\n",
    "\n",
    "# Generate lagged features for the demand from other stores\n",
    "for lag in [7, 14, 28, 364]:  # Specify the lag periods\n",
    "    data[f'demand_other_stores_lag_{lag}'] = (\n",
    "        data.groupby('store_id')['demand_other_stores'].shift(lag)\n",
    "    )\n",
    "\n",
    "# Drop the intermediate column if not needed\n",
    "data.drop(columns=['total_sales_all_stores'], inplace=True)\n",
    "data.drop(columns=['demand_other_stores'], inplace=True)\n",
    "#data.drop(columns=['level_0'], inplace=True)\n",
    "data.drop(columns=['d'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a3e1d135-8322-468d-9298-703433ca91dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15770 entries, 0 to 15769\n",
      "Data columns (total 39 columns):\n",
      " #   Column                        Non-Null Count  Dtype         \n",
      "---  ------                        --------------  -----         \n",
      " 0   index                         15770 non-null  int64         \n",
      " 1   store_id                      15770 non-null  object        \n",
      " 2   y                             15770 non-null  float64       \n",
      " 3   sell_price                    15770 non-null  float64       \n",
      " 4   ds                            15770 non-null  datetime64[ns]\n",
      " 5   lagged_1_price                15770 non-null  float64       \n",
      " 6   lagged_3_price                15770 non-null  float64       \n",
      " 7   lagged_7_price                15770 non-null  float64       \n",
      " 8   lag_7                         15770 non-null  float64       \n",
      " 9   lag_14                        15770 non-null  float64       \n",
      " 10  lag_21                        15770 non-null  float64       \n",
      " 11  lag_28                        15770 non-null  float64       \n",
      " 12  lag_364                       15770 non-null  float64       \n",
      " 13  roll_mean_7                   15770 non-null  float64       \n",
      " 14  roll_std_7                    15770 non-null  float64       \n",
      " 15  roll_mean_14                  15770 non-null  float64       \n",
      " 16  roll_std_14                   15770 non-null  float64       \n",
      " 17  roll_mean_28                  15770 non-null  float64       \n",
      " 18  roll_std_28                   15770 non-null  float64       \n",
      " 19  lagged_1_rolling_sales_slope  15749 non-null  float64       \n",
      " 20  month                         15770 non-null  int32         \n",
      " 21  day_of_month                  15770 non-null  int32         \n",
      " 22  day_of_week                   15770 non-null  int32         \n",
      " 23  is_thanksgiving               15770 non-null  bool          \n",
      " 24  is_christmas                  15770 non-null  bool          \n",
      " 25  sales_store_CA_1              14184 non-null  float64       \n",
      " 26  sales_store_CA_2              14184 non-null  float64       \n",
      " 27  sales_store_CA_3              14184 non-null  float64       \n",
      " 28  sales_store_CA_4              14184 non-null  float64       \n",
      " 29  sales_store_TX_1              14184 non-null  float64       \n",
      " 30  sales_store_TX_2              14184 non-null  float64       \n",
      " 31  sales_store_TX_3              14184 non-null  float64       \n",
      " 32  sales_store_WI_1              14184 non-null  float64       \n",
      " 33  sales_store_WI_2              14184 non-null  float64       \n",
      " 34  sales_store_WI_3              14184 non-null  float64       \n",
      " 35  demand_other_stores_lag_7     15700 non-null  float64       \n",
      " 36  demand_other_stores_lag_14    15630 non-null  float64       \n",
      " 37  demand_other_stores_lag_28    15490 non-null  float64       \n",
      " 38  demand_other_stores_lag_364   12130 non-null  float64       \n",
      "dtypes: bool(2), datetime64[ns](1), float64(31), int32(3), int64(1), object(1)\n",
      "memory usage: 4.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3b6e6b13-b6cc-4279-88ee-e7ee23bf4c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Stores: 100%|████████████████████████████████████████████████████████████| 10/10 [01:37<00:00,  9.75s/store]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Store  Average MASE\n",
      "0              CA_1      0.743078\n",
      "1              CA_2      1.413937\n",
      "2              CA_3      0.446333\n",
      "3              CA_4      1.077713\n",
      "4              TX_1      0.912367\n",
      "5              TX_2      0.609341\n",
      "6              TX_3      0.832145\n",
      "7              WI_1      1.224661\n",
      "8              WI_2      0.585380\n",
      "9              WI_3      0.537057\n",
      "10  Overall Average      0.838201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function for leave-one-out cross-validation and store-wise performance with MASE\n",
    "def loocv_lgb_by_store_with_mase(data, target_col, features, seasonality=7, h=1, m=10):\n",
    "    store_performance = []\n",
    "    \n",
    "    # Initialize progress bar for stores\n",
    "    store_iterator = tqdm(data['store_id'].unique(), desc=\"Processing Stores\", unit=\"store\")\n",
    "    \n",
    "    # Iterate over each store\n",
    "    for store_id in store_iterator:\n",
    "        mase_scores = []\n",
    "        \n",
    "        # Filter and sort data for the current store\n",
    "        store_data = data[data['store_id'] == store_id].sort_values('ds').reset_index(drop=True)\n",
    "        \n",
    "        # Perform leave-one-out cross-validation for the last m periods\n",
    "        for i in range(len(store_data) - m, len(store_data)):\n",
    "            train_data = store_data.iloc[:i]  # Train up to point i\n",
    "            val_data = store_data.iloc[i:i + h]  # Predict h steps ahead\n",
    "            \n",
    "            # Ensure validation data exists\n",
    "            if len(val_data) < h:\n",
    "                continue\n",
    "            \n",
    "            train_x = train_data[features]\n",
    "            train_y = train_data[target_col]\n",
    "            val_x = val_data[features]\n",
    "            val_y = val_data[target_col]\n",
    "            \n",
    "            # LightGBM model\n",
    "            train_dataset = lgb.Dataset(train_x, label=train_y, categorical_feature=['month', 'day_of_month', 'day_of_week'])\n",
    "            \n",
    "            # Define LightGBM parameters\n",
    "            params = {\n",
    "                \"objective\": \"regression\",\n",
    "                \"metric\": \"mae\",\n",
    "                \"learning_rate\": 0.1,\n",
    "                \"max_depth\": 6,\n",
    "                \"num_leaves\": 31,\n",
    "                \"verbosity\": -1\n",
    "            }\n",
    "            \n",
    "            # Train the model\n",
    "            model = lgb.train(\n",
    "                params,\n",
    "                train_dataset,\n",
    "                num_boost_round=1000\n",
    "            )\n",
    "            \n",
    "            # Validation predictions\n",
    "            val_preds = model.predict(val_x)\n",
    "            \n",
    "            # Calculate MASE\n",
    "            mae = np.mean(np.abs(val_y - val_preds))\n",
    "            naive_forecast = train_y.shift(seasonality).iloc[seasonality:]\n",
    "            naive_errors = np.abs(train_y[seasonality:] - naive_forecast)\n",
    "            scaling_factor = naive_errors.mean()\n",
    "            mase = mae / scaling_factor if scaling_factor > 0 else np.nan\n",
    "            mase_scores.append(mase)\n",
    "        \n",
    "        # Store average MASE for the store\n",
    "        store_performance.append({\n",
    "            \"Store\": store_id,\n",
    "            \"Average MASE\": np.nanmean(mase_scores)  # Handle NaN values if no valid MASE\n",
    "        })\n",
    "    \n",
    "    # Convert results to a DataFrame\n",
    "    performance_df = pd.DataFrame(store_performance)\n",
    "    \n",
    "    # Calculate overall average MASE\n",
    "    overall_mase = performance_df['Average MASE'].mean()\n",
    "    performance_df = pd.concat([\n",
    "        performance_df,\n",
    "        pd.DataFrame([{\"Store\": \"Overall Average\", \"Average MASE\": overall_mase}])\n",
    "    ], ignore_index=True)\n",
    "    \n",
    "    return performance_df\n",
    "\n",
    "# Features and target\n",
    "target = 'y'\n",
    "features = [col for col in data.columns if col not in ['ds', 'y', 'store_id']]\n",
    "\n",
    "# Run leave-one-out cross-validation and get MASE performance per store\n",
    "performance_df = loocv_lgb_by_store_with_mase(data, target, features, seasonality=7, h=14, m=28)\n",
    "\n",
    "# Display the performance table\n",
    "print(performance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5451f33d-4dec-4b14-a59f-f84b2a51becd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
